---
date   : "`r Sys.Date()`"
Author : "Byungju Kim"
runtime: shiny
output : 
  bookdown::html_document2: 
    css: css/style.css
    toc: yes
    highlight: espresso
    toc_depth: 4
    toc_float: yes
    code_folding: hide
    df_print: kable
---

```{r setup, include=FALSE}
 #require(tufte)

 require(tidyverse)
 require(bookdown)
 require(HistData)
 require(readMLData)
 require(DiagrammR)
 require(ggplot2)
 require(gridExtra)

 options(width = 1200)

```

# EDGC ML study group

<div style="background-color:#FFFFFF;opacity:1; position:fixed; bottom:0px; right:10px left:10px top:10px; width:100%; height:140px;">
<hr widht="100%" style="border-top: dashed 1px;"/>

  <img src="images/EDGC_LOGO_EN.png" align="left"> &nbsp; <b>Byungju Kim </b> Ph.D. <br />
  &nbsp; Research Scientist / Bioinformatics Team <br />
  &nbsp; # 291 Harmony-ro, Yeonsu-gu, Incheon <br />
  &nbsp; mobile : +82-10-5091-8731 <br />
  &nbsp; email : byungju.kim@edgc.com 
  
</div>
<div class="container">

  <img src="images/ML.png">

  <h5><span>
      Chapter 4 : &nbsp; Modeling and its evaluation <br \>
      Author &nbsp; &nbsp; &nbsp; : &nbsp; Byungju Kim <br \>
      Date &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; : &nbsp; 2018-10-17
  </span></h5>
  
 </div>

<!--
 Greetings, Everyone. this is first meeting of Machine Learning study group.
 I'm the guy who

-->

# Overview on modeling

## What is the Model ?

here are short reviews on previous study.

**Model**

* Ouput of the Machine learning.

* what is being learned from data, in order to solve given task.


<image src="./images/TomWEF2017.JPG" align=right style="margin-right: 10px;"> 

<blockquote>  <span class="Big-first-letter">"</span>A computer program is said to learn from experience **E** with respect to some task **T** and some performance measure **P**, if its performance on **T**, as measured by **P**, improves with experience **E**.<br /><br .> --- **Tom Mitchell (1951-present)** </blockquote>

**Tasks (T)**

* Classification (binary, multiclass classification)

* Regresson 

**Experience (E)** <- today's main topic.

* more data, more modeling

**Performacne (P)**


## Kinds of Modeling

* Geomatric, instance-based Modeling

  * k-Nearleast neighbor
  
  * distance based clustering
  
    * k-mean
    
    * Hierarchical clustering

* Linear Modeling
  
* Probability model

    * naive bayes
    
    * Expectation-Maximisation
    
    * Gaussian mixture models
    
    
# Modeling

Understaning how ML works is helpful to find proper models, algorithm, and optimize parameters.

in this chapter, We learns the following topics

* OLS (Ordinary Least Square), Lasso regression, ridge regression, elastic net regression, stepwise regression

* Parameter optimization using `Gradient Descent`, and it's variables such as `Mini-batch` and `Stoachastic Gradient descent`.

I add some concept and details in the slies.


| &nbsp;         | Logistic Regression | Linear Regression | Ridge | Lasso | Elastic Net |
| :---:          | :---: | :---: | :---: | :---: | :---: |
| Model          |       | $y = \theta_{0} + \theta_{1} x + \epsilon$ |       |       |       |
| Loss Function  |       | $\sum\hat{y}_{i}-y_{i}$ | L2  | $\sum\hat{y}_{i}-y_{i}$ (L1) |       |
| Regularization |       |       |       |       |       |
| Notes          |       |       |       |       |       |
|                |       |       |       |       |       |


| Regression model | equation |
| :--- | :---: |
| Simple Linear Regression | \@ref(eq:SLR) |
| multiple linear regression | ... |
| multivariate linear regression | ... |

**advanced topic**

| Regression Model | equation | 
| :--- | :---: |
| Lasso | | 
| :--- | :---: |
| ridge | |
| elastic net  | |


**Loss function - Classification (Logistic)**
(https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0)

* Log Loss

* Focal Loss

* KL Divergence/Relative Entropy

* Exponential Loss

** Loss Function - Regression **

* Mean Square Error (MSE) / Quadratic Loss, **L2 Loss**

Mean Square Error (MSE) is the most commonly used regression loss function. MSE is the sum of squared distances between our target variable and predicted values.

$$ MSE = \sum_{i=1}^{n} \frac{(\hat{y}_{i}-y_{i})^2}{n}$$

* Mean Absolute Error (MAE, L1 Loss)

Mean Absolute Error (MAE) is another loss function used for regression models. MAE is the sum of absolute differences between our target and predicted variables. So it measures the average magnitude of errors in a set of predictions, without considering their directions. (If we consider directions also, that would be called Mean Bias Error (MBE), which is a sum of residuals/errors). The range is also 0 to ???.

$$ MAE = \sum_{i=1}^{n} \frac{|\hat{y}_{i}-y_{i}|}{n}$$

* MSE vs MAE ( L2 versus L1 loss)

**MAE** - more robust outliers. 
      - useful when training data is corrputed with outliers.
      - it's gradient is same throught, 
**MSE** : easier to solve 

* Huber Loss / smooth Mean Absolute Error

Huber loss is less sensitive to outliers in data than the squared error loss. It???s also differentiable at 0. It???s basically absolute error, which becomes quadratic when error is small. How small that error has to be to make it quadratic depends on a hyperparameter, ???? (delta), which can be tuned. Huber loss approaches MAE when ???? ~ 0 and MSE when ???? ~ ??? (large numbers.)

* Log-cosh Loss

Log-cosh is another function used in regression tasks that???s smoother than L2. Log-cosh is the logarithm of the hyperbolic cosine of the prediction error.

* Quantile Loss



These techniques are mostly driven by 3 metics.

* Number of independent variables

* Shape of the regression

* Type of dependent variable.

## Linear Modeling

### Linear Regrssion

**Exact approach vs Approximation**

* The concept of Random Error introduced only 400 years ago. Before the concept is generalized, the measurement which having small random errors were utterly wrong. Now the errors are considered as approximately trues. it opened up the possibility of investigating approximate, stochastic relationships between variables.

<table>
<tr><td> Height <br /> Prediction <br /> Image </td><td>
```{r height, fig.width=4,fig.height=3,fig.cap="**density plot for 20 height measurements**. What is True height of the person?"}
        
set.seed(1234)
dmat<-rnorm(20,mean=78,sd=0.5)
dmat<-data.frame(height=dmat)
p <- ggplot(dmat,aes(height)) + geom_density(fill="#B00D23") + theme_bw()
p <- p + xlab("measured height, cm") + ggtitle("Density plot of height measurements \nfor annonymous person 1")
p

```
</td></tr></table>

* When we want to know if one measurement variable is associated with another one, then we can plot them to check whether there is linear trends.




### Simple Linear regression

Simple linear regression is an approach for predicting a quantitative response (dependent variables,ouput) using a single feature (single predictor or input variables). simple linear regression fits a straight line to the data.

e.g.) subsampling of data from McDonald's work  (1989).

```{r regression.data,cols.print=3, rows.print=6}
egg<-c(20,23,22,20,25,25,17,24,20,27,24,21,22,22,23,35,26,23,25,24,19,21,20,33,17,21,25,22)
weight<-c(5.38,7.36,6.13,4.75,8.10,8.62,6.30,7.44,7.26,7.17,7.78,6.23,5.42,7.87,5.25,7.37,8.01,4.92,7.03,6.45,5.06,6.72,7.00,9.39,6.49,6.34,6.16,5.74)

df <- data.frame(egg=egg,weight=weight,pred_egg=10.386+weight*1.892)
df <-as.tibble(df)
df %>% select(weight,egg) %>% head(n=5)
str(df)

```

<center>
```{r LinearRegression,fig.width=10,fig.height=4,fig.cap="**Relationship between dry weight and number of eggs female *Platorchestia platensis*,a kind of sand flea, carrying**. suppose that we are interesting in the relationship between female weight and the number of eggs."}

p1<-ggplot(df,aes(weight,egg)) # geom_smooth(method=lm,linetype="dotted") 
p1<-p1+ geom_point(shape=4,stroke=2,color="grey31") + theme_bw() +xlab("Dry Weight, mg") + ylab("Number of egg")
p1<-p1+ggtitle("egg vs dry weights")

p2<-ggplot(df,aes(weight,egg)) + geom_smooth(method=lm,linetype="dotted",se=FALSE,color="red") 
p2<-p2+ geom_point(shape=4,stroke=2,color="grey31") + theme_bw() +xlab("Dry Weight, mg") + ylab("Number of egg") 
p2<-p2+geom_point(data=df[c(1,16,24),],aes(weight,pred_egg),color="red",stroke=2,shape=4)
p2<-p2+annotate("segment",x=5.38,xend=5.38,y=20,yend=20.56496,color="dodgerblue3",size=2,alpha=0.5)
p2<-p2+annotate("segment",x=7.37,xend=7.37,y=24.33004,yend=35,color="dodgerblue3",size=2,alpha=0.5)
p2<-p2+annotate("segment",x=9.39,xend=9.39,y=33,yend=28.15188,color="dodgerblue3",size=2,alpha=0.5)
p2<-p2+ggtitle("Simple linear regression")

grid.arrange(p1,p2,ncol=2)


```
</center>

* Linear relationship is described as $\hat{Y} = \theta_{0} + \theta_{1} x + \epsilon$ where $\theta_{0}$ for <span id="winered">intercept</span> and $\theta_{1}$ for <span id="winered">slope</span> and $\epsilon$ denotes the error. we are trying to find those <span id="winered">regression coefficients</span>

* Linear model is seeking the <span class="redwine">estimaters of linear function<span> between <span class="redwine">a continuous reponse variable (Y)</span> and <span class="redwine">one or more predictor variable (X)</span>

<!--
* Regression problem is supevised learning problems in which the response is continuous.

Linear models are 

- **Parametetric** : 
- **stable** - small variation in training data have only limited impact on the learned model.
- **less likely to overfit the training data than some other models** since they do not have many parameters
- could have low variance and high bias.


Benefits : 

* widely uesd
* runs fast
* easy to use
* hgihly interpretable
* basis for many other methods.

**kinds of Lineal model**

Here are some regression.

* Linear Regression
* Logistic Regression
* Polynomial Regression
* Stepwise Regression
* Ridge Regression
* Lasso Regresson
* ElasticNet regression

these techniques are mostly driven by 3 metics.

* number of independent variables
* shape of the regression
* type of dependent variable.
-->



$$
\begin{equation}
\begin{aligned}
Y &= \theta_{0} + \theta_{1} x + \epsilon \\
  &= \hat{Y} + \epsilon 
\end{aligned}
(\#eq:SLR)
\end{equation}
$$

* Y : Acutual (ground truth) output.
* $\hat{Y}$ : predicted value, output.
* x : feature
* $\theta_{0}$ is the intercept
* $\theta_{1}$ is the coefficient for $x$ (slope of the line). if the predictor changes 1 point, the 
* $\epsilon$ is the error.
* $\\theta_{0} + \theta_{1} x$ : explained proportion.


**How to choose a good model ?**

![figure 3-3. What is a good model ?](./images/whatisgoodSLR.JPG)

* as shown in figure 3-2, Linear model makes a prediction for each observed data point. the observation (y) is predcted by the output of the \@ref(eq:SLR).

* However, we can draw lots of lines shown in figure 3-3. then, what is a good model ?.

We can think the estimation like below :

![figure 3-3.**SLR**](./images/SLR_explain.JPG)

Every dependent variable could have its own real value and predicted values. the differences between real value and predicted value are out prediction/model errors. error for each point is defined by :

$$ 
\begin{equation}
\begin{aligned}
\epsilon &= y - \hat{y} \\
         &= y - (\theta_{0} + \theta_{1} x)
\end{aligned}
(\#eq:error)
\end{equation}
$$
<span class="redwine">**Sum of Squared Error (SSE)**</span>

* The model is better as lower the total error. this is sum of errors. so, model is estimated by <span class="redwine">least square criterion which minimize the sum of squared residuals/errors</span> written as :

$$
\begin{equation}
\begin{aligned}
 SSE &= \sum_{i=1}^{n} (y_{i}-f(x_{i}))^2 \; \; \; \; \; \; \; \text{in the case of one explanatory variable.}
\end{aligned}
(\#eq:SSE)
\end{equation}
$$

Where **n for sample size**, $f(x)$ is the linear model expressed as $f(x)=\hat{y}=(\theta_{0} + \theta_{1} x)$.


**Estimating the coefficients**   

$$X \ni \{1,2,3\}$$
$$Y \ni \{2,4,6\}$$
```{r}
x<-1:3
y<-x*2
```




$$ 
\begin{equation}
\begin{aligned}
SSE &= \sum (Y - f(x))^2 \\
    &= \sum (Y^2 - 2 f(x) \cdot Y + f(x)^2) \\
    &= \sum (Y^2 - 2 (\theta_{0}+\theta_{1}X ) + (\theta_{0}+\theta_{1}X )^2 \\
    &= \sum (Y^2 - 2 \theta_{0}-2 \theta_{1} X + \theta_{0}^2 + 2\theta_{0}\theta_{1}X + \theta_{1}^{2}X^{2})
\end{aligned}
\end{equation}
$$
**calculating \theta_{1}**

$$
\begin{equation}

\begin{aligned}
\frac{d \cdot SSE}{d \cdot \theta_{1}} = 0 &= \sum (0 - 2XY - 0 + 2 \theta_{0}X + 2X^2\theta_{1}) \\
                                           &= \sum (-2XY +2 \theta_{0}X+ 2X^2\theta_{1}) \\
                                           &= \sum (-XY + \theta_{0}X +2X2\theta_{1})
\end{aligned}
\end{equation}
$$

**calculating \theta_{0}**

$$
\begin{equation}

\begin{aligned}
\frac{d \cdot SSE}{d \cdot \theta_{0}} = 0 &= \sum (0 - 2Y + 2 \theta_{1} + 2\theta_{0} + 0) \\
                                            &= \sum (-2Y+2\theta_{1}+2\theta_{0}) \\
\end{aligned}
\end{equation}
$$

$$
\begin{equation}
\begin{aligned}

\therefore \theta_{0}=\sum (Y - \theta_{1}X)

\end{aligned}
\end{equation}
$$



(for proof, see ttps://www.amherst.edu/system/files/media/1287/SLR_Leastsquares.pdfh)


$$ \theta_{1} = \frac{\sum_{i=1}^{n} (X_{i} - \bar{X}) (Y_{i} - \bar{Y})} { \sum_{i=1}^{n} (X_{i}-\bar{X})^2}$$

$$ \theta_{0} = \bar{Y} - \theta_{1} \bar{x} $$




<span class="redwine">**Sum of Squares Regression (SSR)**</span>

* then, how many proportion is explained by the model ?.

the variance of the population is defined by $Var(T) = \sum(\bar{Y}-x_{i})^2$ which measure how far a set of numers are spread out from their average value. by substracting predicted value ($\har{Y}$) from mean of the actual values, we can measure how our model could explaine the variance of the real data by following :





$$
\begin{equation}
 SSR = \sum_{i=1}^{n} (\bar{Y}-\hat{y_{i}})^2 \; \; \; \; \; \; \; \text{in the case of one explanatory variable.}
(\#eq:SSR)
\end{equation}
$$



Oldinary Least Square 


from the dataset,

```{r}
 df

```

$$ 20= \theta_{0} +  \theta_{1} \times 5.38 $$
$$ 23= \theta_{0} +  \theta_{1} \times 7.36 $$
$$ 22= \theta_{0} +  \theta_{1} \times 6.13 $$

$$ 20 = \theta_{0} + \theta_{1} \times 4.75$$







**SSE**

**SSTo**

**SS**




```{r}
        set.seed(2018)
        dmat<-rnorm(100,mean=5,sd=3)
        dmat<-data.frame(X=rep(1:25,each=2)-dmat[1:50],Y=rep(1:25,each=2)-dmat[51:100])
        p<-ggplot(dmat,aes(X,Y))+geom_point(stroke=2,shape=4,color="darkgray") + theme_bw() + geom_smooth(method=lm)
        p<-p+geom_point(data=dmat[c(17,28,30),],aes(X,Y),color="red",inherit.aes = T,shape=4,stroke=2)
        p


```



#### Multiple linear regression

$$\begin{equation} 
        \hat{Y} = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + ... + \theta_{p}x_{p} + \epsilon 
        (#eq:LM) 
\end{equation}$$

* Y : predicted value
* p : number of variables
* $x_{i}$ : value of ith variable.
* $\theta_{j}$ : model parameter for jth variable

the fomula is written as vector format.

\begin{equation} 
        \hat{Y} = h_{\theta}(X) = \theta^{T} \cdot x
        (#eq:LMmat) 
\end{equation}

* $\theta$ : is model parameter from $\theta_{0}$ to $\theta_{n}$
* $\theta^{T}$ : transposed matrix for theta. 1xp matrix
* X : is the feature vector containing $X \ni \{x_{0} x_{0} ... x_{p}\}$. $X_{0}$ is always 0 (intercept)
* \theta^{T} \cdot x
* $h_{\theta}$ is the hypothesis function using model $\theta$


**Limitation of Linear Regression**

* Sensitive to outlier.

* Overfitting

* it's easy to overfit your model such that your regression begins to model the random error in the data,
  (Linear regression is not defined when p > n)

* does not handle non-linear relationship.

### Ganeralized additive model

#### Multivariate linear regression


<!-- Regression Shrinkage Methods -->


### Regression Shrinkage Methods 

**Motivations**

* too many predictors - 

* 

* Variable selection

* stepwise procedure in regression

  * Backward elimination : eliminate the least important variable from the selected one.

  * Forward selection    : add the most important variable from the remaining ones.

  * 


#### Ridge Regression




**Ridge Regresison**

* Ridge Regression has shirinkage term : shrinks the estimate of the coefficients toward zero.

* $\lambda$, <span id="redwine"> tuning parameter</span>, is another therm to control shirinkage term.


### Lasso 

**Least absolute shrinkage and selection operator (LASSO)** is regression analysis method which performs feature selection and regularization in order to enhance prediction accuracy annd interpretatibility of the statistial model.

* made Prediction selection

*

#### Elastic Net regression

* it overcomes the limitaiton of lasso by applying $l^2$ penaty term giving

$$ min {||} $$


#### Fused lasso

*
#### Quasi-morms and bridge regression

*

### OLS vs RIDGE vs LASSO vs Elastic Net

```{r}
# swiss <- datasets::swiss
# x <- model.matrix(Fertility~., swiss)[,-1]
# y <- swiss$Fertility
#lambda <- 10^seq(10, -2, length = 100)

#library(glmnet)

#set.seed(489)
#train = sample(1:nrow(x), nrow(x)/2)
#test = (-train)
#ytest = y[test]

#OLS
#swisslm <- lm(Fertility~., data = swiss)
#coef(swisslm)
#summary(swisslm)

#ridge
#ridge.mod <- glmnet(x, y, alpha = 0, lambda = lambda)
#predict(ridge.mod, s = 0, exact = T, type = 'coefficients')[1:6,]

```

#### ElasticNet

-

### parameter Optimization

**Gradient descent (GD)** 

**GD** is a first-order iterative optimization algorithm for finding the minimum of a function.

### Model evaluation/validation






## Referring Equations

\@ref(eq:LM)
\@ref(eq:LMmat)
\@ref(eq:vector)

## Glossary

* vector : 1 dimensional array,n x 1 matrix. written as

$$ \begin{equation} 
        X = \begin{pmatrix}
                x_{1} \\
                x_{2} \\
                \vdots \\
                x_{n} \\ 
            \end{pmatrix}
            (#eq:vector)  
\end{equation} $$

* $x_{i,j}$ is element of an X matrix, representing the value of the jth variable for the
ith observation, where i = 1, 2,...,n and j = 1, 2,...,p. matrix and it's elements are written as

$$ X = \left( \begin{array} {rrrr}
x_{11} & x_{12} & \cdots & x_{1p} \\
x_{21} & x_{22} & \cdots & x_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n1} & \cdots & \cdots & x_{np} \\
\end{array} \right)
$$


* ^T^ notation defines the transpose of a matrix of vector.

$$ X^{T} = \left( \begin{array} {rrrr}
x_{11} & x_{21} & \cdots & x_{n1} \\
x_{12} & x_{22} & \cdots & x_{n2} \\
\vdots & \vdots & \ddots & \vdots \\
x_{1p} & \cdots & \cdots & x_{np} \\
\end{array} \right)
$$


## Support vector machine

C-classification
nu-classification
one-classification (for novelty detection)
eps-regression
nu-regression
degree

## shiny excercise

```{r eruptions, echo=FALSE}
selectInput(
  'breaks', label = 'Number of bins:',
  choices = c(10, 20, 35, 50), selected = 20
)

renderPlot({
  par(mar = c(4, 4, .1, .5))
  hist(
    faithful$eruptions, as.numeric(input$breaks),
    col = 'gray', border = 'white',
    xlab = 'Duration (minutes)', main = ''
  )
})



